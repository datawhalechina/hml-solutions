# 第12章 决策树

# 习题1
以下关于决策树的说法错误的是：   
&emsp; A. 决策树选择具有更强分类能力的特征进行分裂。    
&emsp; B. 决策树只能处理分类问题，无法处理连续的回归问题。   
&emsp; C. 决策树可以解决非线性可分的分类问题。   
&emsp; D. 决策树的预测结果分布在叶子节点上。  

**解答**

**答案：B**  
&emsp; &emsp; A. 正确。决策树选择最优的特征作为划分标准进行分裂，从而提高模型的准确性。  
&emsp; &emsp; B. 错误。决策树可以处理连续的回归问题，将连续的特征划分为离散的区间就可以解决回归的问题了。  
&emsp; &emsp; C. 正确。决策树是非参数模型，根据数据的分布来选择合适的特征分类，解决非线性可分的分类问题。  
&emsp; &emsp; D. 正确。决策树的内部节点是判断条件，叶子节点是预测/分类结果。

## 习题2
以下关于ID3算法的说法错误的是：   
&emsp; A. 由ID3算法构建的决策树，一个特征不会在同一条路径中出现两次。   
&emsp; B. 作为非参数化模型，ID3算法不会出现过拟合。   
&emsp; C. 在节点分裂出来的树枝数目取决于分类特征可能取值的数量。   
&emsp; D. 信息增益率为信息增益与熵之间的比值，可以排除特征本身复杂度的影响。  

**解答**

**答案：B**  
&emsp; &emsp; A. 正确。ID3算法每个特征只会一条路径上出现一次，避免重复使用。  
&emsp; &emsp; B. 错误。理论上树的深度过大和叶子节点过多能逼近到数据的分布，导致过拟合。需要剪枝机制处理。  
&emsp; &emsp; C. 正确。节点分裂树枝的数目取决于分类特征取值的数量。如果是分类问题，那么分类的类别就是树枝数，如果是回归问题，会对取值进行离散化处理，划分成不同的区间进行取值。    
&emsp; &emsp; D. 正确。信息增益是特征对目标变量的信息贡献，熵是信息的复杂度，信息增益率能更好的衡量特征对目标变量的信息贡献，避免选择复杂度很高，增益很小的特征，排除复杂度的影响。

## 习题3
设$X$和$Y$是相互独立的随机变量，证明：
   $$H(XY)=H(X)+H(Y) \\ H(XX)=H(X)$$  
其中$H(XY)$表示变量$X$和$Y$的联合熵，是基于其联合分布$p(X,Y)$而计算的。

**解答**


**证明：**

熵：$H(X)=-\sum_{x}p(x)\log{p(x)}$  
联合熵：$H(XY)=-\sum_{x}\sum_{y}p(x,y)\log{p(x,y)}$  

- 
$$
\begin{aligned} 
H(XY) 
&= -\sum_{x}\sum_{y} p(x,y) \log p(x,y) \\ 
&= -\sum_{x}\sum_{y} p(x)p(y) \log (p(x)p(y)) \\ 
&= -\sum_{x}\sum_{y} p(x)p(y) (\log p(x) + \log p(y)) \\ 
&= -(\sum_{x}p(x)\log p(x)\sum_{y}p(y))-(\sum_{y}p(y)\log p(y)\sum_{x}p(x)) \\
&其中\sum_{x}p(x) = \sum_{y}p(y) = 1 \\
&= -(\sum_{x} p(x) \log p(x)) - (\sum_{y} p(y) \log p(y)) \\ 
&= H(X) + H(Y) \end{aligned}
$$

-  不会
$p(X,X)=p(X=x)p(X=x)=p(X)^2$
$$
\begin{align*}
H(XX) 
&= \sum_{x}\sum_{x}p(x,x)\log p(x,x) 
\end{align*}
$$

## 习题4
在12.1节的例子中，计算用湿度为标准进行一次分类的信息增益和信息增益率。

**解答**

 - 用湿度分类：记湿度高为$Y_H$，湿度中为$Y_M$。
  - 湿度高样本8个，其中外出2个，未外出6个，得
    $$H(X|Y_H)=-\frac{6}{8}\log\frac{6}{8}-\frac{2}{8}\log\frac{2}{8}\approx 0.8113$$
  - 湿度中样本6个，外出5个，未外出1个，得
    $$H(X|Y_M)=-\frac{5}{6}\log\frac{5}{6}-\frac{1}{6}\log\frac{1}{6}\approx 0.6500$$
  - 条件熵
    $$H(X|Y)\approx H(X|Y_H)\times\frac{8}{14} + H(X|Y_M)\times\frac{6}{14}\approx 0.7422$$
  - 信息增益
    $$I(X|Y)\approx H(X)-H(X|Y)\approx 1-0.7422\approx 0.2578$$
  - 分布的熵
    $$H_Y(X) = -\frac{8}{14}\log\frac{8}{14}-\frac{6}{14}\log\frac{6}{14}\approx 0.9853$$
  - 信息增益率
    $$I_R(X,Y) = \frac{I(X|Y)}{H_Y(X)}\approx \frac{0.2578}{0.9853} \approx 0.2616$$


## 习题5
在本章C4.5决策树代码的基础上实现CART分类树。

**解答**

- 不纯度度量方法更改
  - C4.5：信息增益比
  - CART：基尼不纯度
- 节点分裂方法更改
  - C4.5：可分裂多个子节点
  - CART：二叉分裂，两个子节点
- 叶结点表示类别更改
  - C4.5：多个类别
  - CART：单一类别
- 剪枝策略更改
  - C4.5：预剪枝/后剪枝
  - CART：后剪枝
- other
  - 数据集分割（split_dataset）：根据特征和阈值将数据集分割成两个子集，这是CART决策树分裂过程中的一个关键步骤。

寻找最佳分裂点（find_best_split）：遍历所有特征和可能的阈值，寻找能够最大程度降低基尼不纯度的最佳分裂点。

构建决策树（build_tree）：递归地构建决策树，根据数据集的当前状态决定是继续分裂还是创建叶节点。

训练（fit）：通过调用build_tree方法来训练决策树。

预测（predict_instance 和 predict）：通过递归地遍历决策树来预测单个或多个样本的类别。

准确率计算（accuracy）：计算模型在给定数据集上的预测准确率。


```python
# 数据预处理代码
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report

# 读取数据
data = pd.read_csv('./titanic/train.csv')
# 删去编号、姓名、船票编号3列
data.drop(columns=['PassengerId', 'Name', 'Ticket'], inplace=True)

# 处理Cabin特征：转换为是否存在的二值特征
data['Cabin'] = data['Cabin'].notna().astype(int)

feat_ranges = {}
cont_feat = ['Age', 'Fare']  # 连续特征
bins = 10  # 分箱数

# 处理连续特征分箱
for feat in cont_feat:
    # 计算分箱边界（忽略缺失值）
    min_val = data[feat].min(skipna=True)
    max_val = data[feat].max(skipna=True)
    bin_edges = np.linspace(min_val, max_val, bins + 1)
    
    # 分箱并替换原始数据
    data[feat] = pd.cut(data[feat], bins=bin_edges, labels=False, include_lowest=True)
    # 处理缺失值为-1
    data[feat] = data[feat].fillna(-1).astype(int)
    feat_ranges[feat] = [-1] + list(range(bins))

# 处理离散特征
cat_feat = ['Sex', 'Pclass', 'SibSp', 'Parch', 'Embarked']
for feat in cat_feat:
    # 转换为分类代码并处理缺失值
    data[feat] = data[feat].astype('category').cat.codes
    # 收集特征取值范围
    unique_values = data[feat].unique().tolist()
    unique_values.sort()
    feat_ranges[feat] = unique_values

# 确保所有缺失值已被处理
data.fillna(-1, inplace=True)

# 划分训练集与测试集（分层抽样）
X = data.drop(columns='Survived')
y = data['Survived']
train_x, test_x, train_y, test_y = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=0
)

print('训练集大小：', len(train_x))
print('测试集大小：', len(test_x))
print('特征数：', train_x.shape[1])

```

    训练集大小： 712
    测试集大小： 179
    特征数： 8
    


```python
# 定义优化的决策树类
class Node:
    __slots__ = ['feature_index', 'threshold', 'left_child', 'right_child', 'predicted_class']
    def __init__(self):
        self.feature_index = None
        self.threshold = None
        self.left_child = None
        self.right_child = None
        self.predicted_class = None

class OptimizedCARTDecisionTree:
    def __init__(self, max_depth=5, min_samples_split=10):
        self.max_depth = max_depth
        self.min_samples_split = min_samples_split
        self.tree = None
        self.T = 0

    def gini(self, Y):
        counts = np.bincount(Y)
        probabilities = counts / len(Y)
        return 1 - np.sum(probabilities ** 2)

    def find_best_split(self, X, Y):
        best_gini = float('inf')
        best_feature = None
        best_threshold = None

        for feature in range(X.shape[1]):
            # 获取唯一值并生成候选阈值
            values = X[:, feature]
            unique_values = np.unique(values)
            if len(unique_values) < 2:
                continue
            
            # 生成候选阈值（相邻值的中间点）
            thresholds = (unique_values[:-1] + unique_values[1:]) / 2
            
            for threshold in thresholds:
                left_mask = values <= threshold
                right_mask = ~left_mask
                
                # 跳过不满足最小样本数的分割
                if left_mask.sum() < self.min_samples_split or right_mask.sum() < self.min_samples_split:
                    continue
                
                # 计算基尼指数
                gini = (self.gini(Y[left_mask]) * left_mask.sum() +
                        self.gini(Y[right_mask]) * right_mask.sum()) / len(Y)
                
                if gini < best_gini:
                    best_gini = gini
                    best_feature = feature
                    best_threshold = threshold

        return best_feature, best_threshold

    def build_tree(self, X, Y, depth=0):
        # 终止条件
        if (len(np.unique(Y)) == 1 or
            (self.max_depth is not None and depth >= self.max_depth) or
            len(Y) < self.min_samples_split):
            leaf = Node()
            leaf.predicted_class = np.argmax(np.bincount(Y))
            self.T += 1
            return leaf
        
        # 寻找最佳分割
        feature, threshold = self.find_best_split(X, Y)
        if feature is None:
            leaf = Node()
            leaf.predicted_class = np.argmax(np.bincount(Y))
            self.T += 1
            return leaf
        
        # 分割数据集
        left_mask = X[:, feature] <= threshold
        right_mask = ~left_mask
        
        # 递归构建子树
        node = Node()
        node.feature_index = feature
        node.threshold = threshold
        node.left_child = self.build_tree(X[left_mask], Y[left_mask], depth+1)
        node.right_child = self.build_tree(X[right_mask], Y[right_mask], depth+1)
        
        return node

    def fit(self, X, Y):
        self.tree = self.build_tree(X, Y)
        
    def predict(self, X):
        return np.array([self._predict(x) for x in X])
    
    def _predict(self, x, node=None):
        if node is None:
            node = self.tree
        if node.predicted_class is not None:
            return node.predicted_class
        if x[node.feature_index] <= node.threshold:
            return self._predict(x, node.left_child)
        else:
            return self._predict(x, node.right_child)
    
    def accuracy(self, X, Y):
        return np.mean(self.predict(X) == Y)

# 训练并评估模型
DT = OptimizedCARTDecisionTree(max_depth=5, min_samples_split=10)
DT.fit(train_x.to_numpy(), train_y.to_numpy())

print('叶节点数量：', DT.T)
print('训练集准确率：', DT.accuracy(train_x.to_numpy(), train_y.to_numpy()))
print('测试集准确率：', DT.accuracy(test_x.to_numpy(), test_y.to_numpy()))
print('测试集报告：')
print("\n")
print(classification_report(test_y, DT.predict(test_x.to_numpy())))
```

    叶节点数量： 24
    训练集准确率： 0.8412921348314607
    测试集准确率： 0.7932960893854749
    测试集报告：
    
    
                  precision    recall  f1-score   support
    
               0       0.81      0.87      0.84       110
               1       0.77      0.67      0.71        69
    
        accuracy                           0.79       179
       macro avg       0.79      0.77      0.78       179
    weighted avg       0.79      0.79      0.79       179
    
    

## 习题6
尝试将决策树应用到第11章支持向量机中用到的linear.csv和spiral.csv分类数据集上。先猜想一下分类效果与支持向量机相比如何，再用实验验证你的猜想。注意，需要先对连续特征离散化。

**解答**

- 对于线性可分的数据，决策树和支持向量机比较好地划分数据，决策树有明显的折线，说明是树的形式划分的
- 对于线性不可分的数据，决策树和支持向量机不好划分数据。调参后能大概拟合。
- 离散化使得这两个数据集不适合训练，故没有离散化。


```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split

# 定义绘制决策边界函数
def plot_decision_boundary(model, X, y, title):
    cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])
    cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])
    
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02), np.arange(y_min, y_max, 0.02))
    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    
    plt.figure()
    plt.pcolormesh(xx, yy, Z, cmap=cmap_light)
    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold, edgecolor='k', s=20)
    plt.xlim(xx.min(), xx.max())
    plt.ylim(yy.min(), yy.max())
    plt.title(title)
    plt.xlabel("Feature 1")
    plt.ylabel("Feature 2")
    plt.show()

# 加载数据
linear_data = pd.read_csv('../第11章 支持向量机/linear.csv')
spiral_data = pd.read_csv('../第11章 支持向量机/spiral.csv')

# 划分特征和标签
y_linear = linear_data.iloc[:, -1]
y_spiral = spiral_data.iloc[:, -1]

# 划分数据集为训练集和测试集
X_linear_train, X_linear_test, y_linear_train, y_linear_test = train_test_split(linear_data.iloc[:, :-1], y_linear, test_size=0.2, random_state=42)
X_spiral_train, X_spiral_test, y_spiral_train, y_spiral_test = train_test_split(spiral_data.iloc[:, :-1], y_spiral, test_size=0.2, random_state=42)

# 创建并训练DecisionTree模型
dt_linear = DecisionTreeClassifier(random_state=42)
dt_linear.fit(X_linear_train, y_linear_train)

dt_spiral = DecisionTreeClassifier(random_state=42)
dt_spiral.fit(X_spiral_train, y_spiral_train)

# 创建并训练支持向量机模型
svm_linear = SVC(kernel='linear', random_state=42)
svm_linear.fit(X_linear_train, y_linear_train)

svm_spiral = SVC(kernel='rbf', gamma=50, tol=1e-6, random_state=42)
svm_spiral.fit(X_spiral_train, y_spiral_train)

# 在测试集上进行预测并计算准确率
dt_linear_pred = dt_linear.predict(X_linear_test)
dt_linear_accuracy = accuracy_score(y_linear_test, dt_linear_pred)

dt_spiral_pred = dt_spiral.predict(X_spiral_test)
dt_spiral_accuracy = accuracy_score(y_spiral_test, dt_spiral_pred)

svm_linear_pred = svm_linear.predict(X_linear_test)
svm_linear_accuracy = accuracy_score(y_linear_test, svm_linear_pred)

svm_spiral_pred = svm_spiral.predict(X_spiral_test)
svm_spiral_accuracy = accuracy_score(y_spiral_test, svm_spiral_pred)

print("linear dataset - DecisionTree Acc:", dt_linear_accuracy)
plot_decision_boundary(dt_linear, X_linear_train.values, y_linear_train, title="linear dataset - DecisionTree")
print("linear dataset - SVM Acc:", svm_linear_accuracy)
plot_decision_boundary(svm_linear, X_linear_train.values, y_linear_train, title="linear dataset - SVM")
print("spiral dataset - DecisionTree Acc:", dt_spiral_accuracy)
plot_decision_boundary(dt_spiral, X_spiral_train.values, y_spiral_train, title="spiral dataset - DecisionTree")
print("spiral dataset - SVM Acc:", svm_spiral_accuracy)
plot_decision_boundary(svm_spiral, X_spiral_train.values, y_spiral_train, title="spiral dataset - SVM")
```

    linear dataset - DecisionTree Acc: 1.0
    

    d:\miniconda\envs\d2l\lib\site-packages\sklearn\utils\validation.py:2739: UserWarning: X does not have valid feature names, but DecisionTreeClassifier was fitted with feature names
      warnings.warn(
    


    
![png](images/ch12_21_2.png)
    


    d:\miniconda\envs\d2l\lib\site-packages\sklearn\utils\validation.py:2739: UserWarning: X does not have valid feature names, but SVC was fitted with feature names
      warnings.warn(
    

    linear dataset - SVM Acc: 1.0
    


    
![png](images/ch12_21_5.png)
    


    spiral dataset - DecisionTree Acc: 0.6666666666666666
    

    d:\miniconda\envs\d2l\lib\site-packages\sklearn\utils\validation.py:2739: UserWarning: X does not have valid feature names, but DecisionTreeClassifier was fitted with feature names
      warnings.warn(
    


    
![png](images/ch12_21_8.png)
    


    spiral dataset - SVM Acc: 0.6410256410256411
    

    d:\miniconda\envs\d2l\lib\site-packages\sklearn\utils\validation.py:2739: UserWarning: X does not have valid feature names, but SVC was fitted with feature names
      warnings.warn(
    


    
![png](images/ch12_21_11.png)
    


## 习题7
假设在一个二维数据的二分类任务中，最优分类边界是$x_1-x_2=0$，但是决策树模型只能沿着坐标轴的方向去切分二维数据空间，这样耗费很多分裂节点也无法取得很好的分类性能，试思考在此类情形下应该如何应对。

**解答**

  &emsp; &emsp; 可以考虑将原始的数据进行线性变换或非线性变换，将数据映射到新的特征空间中去，进行降维。或者添加一些新的特征到数据集里面去，比如将特征加起来，这些新数据可以是原数据的一些能反映特征的数据。实在不行就使用别的模型，使用集成学习等
